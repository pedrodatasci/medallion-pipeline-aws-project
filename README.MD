# üèóÔ∏è Medallion Architecture Pipeline com AWS Glue, S3 e Redshift

Este projeto implementa uma pipeline completa de dados baseada na arquitetura **Medallion** (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Redshift) utilizando apenas **servi√ßos gerenciados da AWS**:

- Coleta de dados p√∫blicos da **API Open Brewery DB**
- Armazenamento em **S3 (Raw, Bronze, Silver)**
- Processamento com **AWS Glue**
- Carga final em **Redshift Serverless**

---

## üîß Tecnologias Utilizadas

- **AWS S3** ‚Äî Armazenamento raw/bronze/silver
- **AWS Glue** ‚Äî Execu√ß√£o de transforma√ß√µes ETL em PySpark
- **AWS Redshift Serverless** ‚Äî Armazenamento anal√≠tico final
- **Terraform** ‚Äî Infraestrutura como c√≥digo
- **Bash + Python** ‚Äî Orquestra√ß√£o da pipeline

---

## üìÅ Estrutura do Projeto

```
medallion-pipeline-aws-project/
‚îÇ
‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îî‚îÄ‚îÄ api_collector.py
‚îÇ
‚îú‚îÄ‚îÄ etl/
‚îÇ   ‚îú‚îÄ‚îÄ bronze_glue.py
‚îÇ   ‚îî‚îÄ‚îÄ silver_glue.py
‚îÇ
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îú‚îÄ‚îÄ iam.tf
‚îÇ   ‚îú‚îÄ‚îÄ bucket.tf
‚îÇ   ‚îú‚îÄ‚îÄ glue_jobs.tf
‚îÇ   ‚îú‚îÄ‚îÄ redshift.tf
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf
‚îÇ   
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ flow.png
‚îÇ
‚îú‚îÄ‚îÄ run_pipeline.sh
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ .gitignore
```

---

## ‚öôÔ∏è Pr√©-Requisitos

- Conta AWS com permiss√µes para Glue, S3, Redshift e IAM
- AWS CLI configurada (`aws configure`)
- Terraform instalado (`>= 1.3.0`)
- Python 3 instalado

---

## üöÄ Como Executar

### 1. Clone o projeto e entre na pasta

```bash
git clone https://github.com/pedrodatasci/medallion-pipeline-aws-project.git
cd medallion-pipeline-aws-project
```

### 2. Provisione a infraestrutura

```bash
cd terraform
terraform init
terraform apply
cd ..
```

> Isso criar√°:  
> - Bucket S3  
> - Jobs Glue Bronze e Silver  
> - Workgroup Redshift Serverless  
> - Role com permiss√µes adequadas

### 3. Execute a pipeline completa
Mas antes: a role IAM utilizada no COPY precisa ser **associada manualmente** ao Redshift Workgroup no console AWS.
Caso contr√°rio a etapa da COPY ir√° falhar.

```bash
chmod +x run_pipeline.sh
./run_pipeline.sh
```

Este script:

- Coleta os dados da API e envia para o S3 (Raw)
- Executa os jobs Glue de Bronze e Silver
- Cria a tabela no Redshift (se n√£o existir)
- Faz o COPY da camada Silver para o Redshift

---

## üß™ Resultados Esperados

Ap√≥s a execu√ß√£o completa, os dados estar√£o dispon√≠veis:

- Em `s3://<bucket>/bronze/` no formato Parquet
- Em `s3://<bucket>/silver/` no formato Parquet
- Na tabela `analytics_db.public.breweries_silver` do Redshift Serverless
    - Pode-se adicionar um schemma no Terraform caso queira

---

## üìä Esquema da Tabela no Redshift

```sql
CREATE TABLE IF NOT EXISTS public.breweries_silver (
    id VARCHAR,
    name VARCHAR,
    brewery_type VARCHAR,
    city VARCHAR,
    state VARCHAR,
    country VARCHAR,
    latitude DOUBLE PRECISION,
    longitude DOUBLE PRECISION,
    website_url VARCHAR,
    data_ingestao TIMESTAMP
);
```

---

## üìå Observa√ß√µes

- A role IAM utilizada no COPY precisa ser **associada manualmente** ao Redshift Workgroup no console AWS.
- O bucket S3 √© nomeado automaticamente via Terraform (`openbrewery-data-<sufixo>`).
- Em produ√ß√£o, recomenda-se utilizar o Secrets Manager para a senha do Redshift.

---

## üìà Diagrama da Pipeline

![Arquitetura](./docs/flow.png)

---

## üßë‚Äçüíª Autor

Pedro S√° ‚Äî [LinkedIn](https://www.linkedin.com/in/pedro-sofiati-de-sa/)

---