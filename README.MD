# ğŸ—ï¸ Medallion Architecture Pipeline com AWS Glue, S3 e Redshift

Este projeto implementa uma pipeline completa de dados baseada na arquitetura **Medallion** (Raw â†’ Bronze â†’ Silver â†’ Redshift) utilizando apenas **serviÃ§os gerenciados da AWS**:

- Coleta de dados pÃºblicos da **API Open Brewery DB**
- Armazenamento em **S3 (Raw, Bronze, Silver)**
- Processamento com **AWS Glue**
- Carga final em **Redshift Serverless**

---

## ğŸ”§ Tecnologias Utilizadas

- **AWS S3** â€” Armazenamento raw/bronze/silver
- **AWS Glue** â€” ExecuÃ§Ã£o de transformaÃ§Ãµes ETL em PySpark
- **AWS Redshift Serverless** â€” Armazenamento analÃ­tico final
- **Terraform** â€” Infraestrutura como cÃ³digo
- **Bash + Python** â€” OrquestraÃ§Ã£o da pipeline

---

## ğŸ“ Estrutura do Projeto

```
medallion-pipeline-aws-project/
â”‚
â”œâ”€â”€ ingestion/
â”‚   â””â”€â”€ api_collector.py
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ bronze_glue.py
â”‚   â””â”€â”€ silver_glue.py
â”‚
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ iam.tf
â”‚   â”œâ”€â”€ bucket.tf
â”‚   â”œâ”€â”€ glue_jobs.tf
â”‚   â”œâ”€â”€ redshift.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ flow.png
â”‚
â”œâ”€â”€ run_pipeline.sh
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## âš™ï¸ PrÃ©-Requisitos

- Conta AWS com permissÃµes para Glue, S3, Redshift e IAM
- AWS CLI configurada (`aws configure`)
- Terraform instalado (`>= 1.3.0`)
- Python 3 instalado

---

## ğŸš€ Como Executar

### 1. Clone o projeto e entre na pasta

```bash
git clone https://github.com/seu-user/medallion-pipeline-aws-project.git
cd medallion-pipeline-aws-project
```

### 2. Provisione a infraestrutura

```bash
cd terraform
terraform init
terraform apply
cd ..
```

> Isso criarÃ¡:  
> - Bucket S3  
> - Jobs Glue Bronze e Silver  
> - Workgroup Redshift Serverless  
> - Role com permissÃµes adequadas

### 3. Execute a pipeline completa

```bash
chmod +x run_pipeline.sh
./run_pipeline.sh
```

Este script:

- Coleta os dados da API e envia para o S3 (Raw)
- Executa os jobs Glue de Bronze e Silver
- Cria a tabela no Redshift (se nÃ£o existir)
- Faz o COPY da camada Silver para o Redshift

---

## ğŸ§ª Resultados Esperados

ApÃ³s a execuÃ§Ã£o completa, os dados estarÃ£o disponÃ­veis:

- Em `s3://<bucket>/bronze/` no formato Parquet
- Em `s3://<bucket>/silver/` no formato Parquet
- Na tabela `analytics_db.public.breweries_silver` do Redshift Serverless

---

## ğŸ“Š Esquema da Tabela no Redshift

```sql
CREATE TABLE IF NOT EXISTS public.breweries_silver (
    id VARCHAR,
    name VARCHAR,
    brewery_type VARCHAR,
    city VARCHAR,
    state VARCHAR,
    country VARCHAR,
    latitude DOUBLE PRECISION,
    longitude DOUBLE PRECISION,
    website_url VARCHAR,
    data_ingestao TIMESTAMP
);
```

---

## ğŸ“Œ ObservaÃ§Ãµes

- A role IAM utilizada no COPY precisa ser **associada manualmente** ao Redshift Workgroup no console AWS.
- O bucket S3 Ã© nomeado automaticamente via Terraform (`openbrewery-data-<sufixo>`).
- Em produÃ§Ã£o, recomenda-se utilizar o Secrets Manager para a senha do Redshift.

---

## ğŸ“ˆ Diagrama da Pipeline

![Arquitetura](./docs/flowchart.png)

---

## ğŸ§‘â€ğŸ’» Autor

Pedro SÃ¡ â€” [LinkedIn](https://www.linkedin.com/in/pedro-sofiati-de-sa/)

---